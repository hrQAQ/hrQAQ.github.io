{"title":"「拥塞控制算法」PCC Vivace阅读笔记","date":"2022-11-07T12:09:21.000Z","date_formatted":{"ll":"Nov 7, 2022","L":"11/07/2022","MM-DD":"11-07"},"thumbnail":"2022/11/07/「拥塞控制算法」PCC Vivace阅读笔记/「拥塞控制算法」PCC-Vivace阅读笔记/ball.bmp","link":"2022/11/07/「拥塞控制算法」PCC Vivace阅读笔记","comments":true,"tags":["PCC","拥塞控制","论文笔记"],"categories":["计算机网络"],"updated":"2022-11-08T20:02:11.752Z","content":"<blockquote>\n<p>PCC Vivace: Online-Learning Congestion Control</p>\n<p>Vivace pronounce as / vɪˈvɑtʃi / not / vɪˈvɑs /</p>\n</blockquote>\n<h2 id=\"abstract\">Abstract<a title=\"#abstract\" href=\"#abstract\"></a></h2>\n<p>作者在PCC的基础上，借助机器学习中在线学习（凸优化）的概念设计了Vivace。相较于PCC而言，Vivace在两个重要的地方做了改进：</p>\n<blockquote>\n<ul>\n<li>rate control protocol【速率控制协议】</li>\n<li>a utility function framework【效用方程框架】</li>\n</ul>\n</blockquote>\n<p>经过理论证明和实验的验证，Vivace具有以下的优点：</p>\n<blockquote>\n<ul>\n<li>Vivace significantly outperforms traditional TCP variants, the previous realization of the PCC framework, and BBR in terms of performance (throughput, latency, loss)【性能比TCP variants、PCC、BBR强】</li>\n<li>convergence speed【收敛速率】</li>\n<li>alleviating bufferbloat【缓解缓冲区膨胀—考虑时延的结果】</li>\n<li>reactivity to changing network conditions【适应高度变化的网络】</li>\n<li>friendliness towards legacy TCP in a range of scenarios. 【对TCP友好】</li>\n<li>Vivace requires only sender-side changes and is thus readily deployable【易于部署】</li>\n</ul>\n</blockquote>\n<h2 id=\"1.introduction\">1.Introduction<a title=\"#1.introduction\" href=\"#1.introduction\"></a></h2>\n<p>作者先定义了一个优秀的拥塞控制算法应该具有的特征（这个定义应该是方便凸显自己的算法的优势的）：</p>\n<blockquote>\n<ul>\n<li>\n<p>First and foremost, a congestion control architecture should be able to <strong>efficiently utilize network resources</strong> under varying and <strong>complex network conditions</strong>. This includes optimizing for <strong>throughput</strong>, <strong>loss</strong>, and <strong>latency</strong>, and doing so in a <strong>plethora of environments</strong> — potentially with non-congestion loss [8], high-RTT cross-continent links, highly dynamic networks such as WiFi and LTE links, etc.【性能要强】</p>\n</li>\n<li>\n<p>Second, congestion control should <strong>guarantee quick convergence to stable and fair rates</strong> when <strong>multiple senders</strong> compete over network resources.【收敛性（快、稳定）和公平性要好，而且要在多个sender的情况下】</p>\n</li>\n<li>\n<p>Last, a congestion control scheme should be easy and safe (e.g., sufficiently friendly to existing protocols) to <strong>deploy</strong>【兼容旧体系】</p>\n</li>\n</ul>\n</blockquote>\n<p>在占据了定义的制高点后，作者开始批判传统算法，例如CUBIC、Illinois、Vegas：</p>\n<blockquote>\n<p>Traditional algorithms [6, 15, 23] fail to satisfy the first two requirements; their performance can be as high as 10× away from the optimal under non-congestion packet loss [11]【传统算法性能只能达到最优性能的不到十分之一】</p>\n</blockquote>\n<p>批判完传统算法之后，又把炮口朝向了最近的新算法，例如Remy、BBR、PCC：</p>\n<blockquote>\n<ul>\n<li>Remy-generated TCPs are inherently prone to <strong>degraded performance</strong> when the actual network conditions deviate from input assumptions.【Remy还残留了TCP的硬连接方案】</li>\n<li>BBR and PCC Both fail to achieve <strong>optimal low latency</strong> and exhibit far-from ideal tradeoffs between <strong>convergence speed and stability.</strong>【BBR和PCC都没有一个好的收敛性】</li>\n<li>BBR exhibits high rate variance and high packet loss rate upon convergence.【BBR稳定性差、收敛时丢包多】</li>\n<li>PCC convergence time is overly long【PCC收敛时间长】</li>\n<li>BBR’s model of the network does not reflect the complexities of reality, performance can suffer severely.【  】</li>\n<li>BBR and PCC are both highly aggressive towards TCP【BBR和PCC都对TCP不友好】</li>\n</ul>\n</blockquote>\n<p>终于，作者把能拥塞算法界的所有算法都狠狠批判了一遍以后，提出了自己的解决方案（I solved this problem），并且给出了自己在这个方案上的主要工作内容（也是本文主要介绍的内容）。</p>\n<p>首先我们需要明确，Vivace是PCC的改进版本。正如摘要中所述，Vivace改进了PCC两个部件：①a utility function framework，② a learning rate control algorithm</p>\n<blockquote>\n<p>Vivace adopts the high-level architecture of PCC – a utility function framework and a learning ratecontrol algorithm – but realizes both components differently.</p>\n</blockquote>\n<p>在第一个部件，即效用方程上，Vivace的贡献在于引进了<strong>learning-theory-informed framework</strong>【所以这个到底是啥啊啊啊啊】来做utility的计算（PCC在效用函数的选择上显得很随意），并且这一次Vivace考虑了<strong>时延最小化</strong>和<strong>TCP友好性</strong>（PCC没有证明考虑时延时的收敛性，PCC没有做到TCP友好性）【我感觉考虑了时延就等于考虑了TCP友好性？？？？】。</p>\n<p>在第二个部件，即速率控制算法上，Vivace使用了基于梯度上升（一种可证明的渐进式在线学习优化）的速率控制算法，达到了：</p>\n<blockquote>\n<ul>\n<li>high utilization of network capacity</li>\n<li>swift reaction to changes</li>\n<li>fast and stable convergence</li>\n</ul>\n</blockquote>\n<h2 id=\"2.rate-control-through-online-learning\">2.Rate-Control Through Online Learning<a title=\"#2.rate-control-through-online-learning\" href=\"#2.rate-control-through-online-learning\"></a></h2>\n<p>因为PCC在异构的效用方程下的结果并不尽如人意，作者从博弈论的无悔算法获得了灵感。无悔算法保证：</p>\n<blockquote>\n<p>In addition, results in game theory establish that online learning algorithms “play well” together, in the sense that, under the appropriate conditions, global convergence to a stable equilibrium is guaranteed when there are multiple decision makers.</p>\n</blockquote>\n<p>同时，在线学习能在不确定的条件下做出最大化正确性决定：</p>\n<blockquote>\n<p>Online learning provides a useful and powerful abstraction for decision making under uncertainty.</p>\n</blockquote>\n<blockquote>\n<p>关于在线学习的背景知识：</p>\n<p>在线学习(Online Learning) - 别来无恙的文章 - 知乎 <a href=\"https://zhuanlan.zhihu.com/p/403393226\" target=\"_blank\">https://zhuanlan.zhihu.com/p/403393226</a></p>\n<p>在线学习（Online Learning）导读 - 吴海波的文章 - 知乎 <a href=\"https://zhuanlan.zhihu.com/p/36410780\" target=\"_blank\">https://zhuanlan.zhihu.com/p/36410780</a></p>\n</blockquote>\n<p>所以作者把rate control的任务抽象为了一个在线学习的任务，这一个insight 在PCC已经出现过了：</p>\n<blockquote>\n<p>A traffic sender repeatedly selects between sending rates. After sending at a certain rate for “sufficiently long”, the sender learns its performance implications by translating aggregated statistics (e.g., achieved goodput, packet loss rate, average latency) into a numerical utility value, and then adapts the sending rate in response.</p>\n</blockquote>\n<p>在15年的PCC模型中，虽然使用了在线学习的理念，但仍然没有挖掘出在线学习的全部潜力：</p>\n<blockquote>\n<p>First, Allegro uses a somewhat arbitrary choice of utility function</p>\n<ul>\n<li>fair convergence is not provably guaranteed when utility functions are latency-aware【考虑时延的效用方程未被证明是公平收敛的】</li>\n<li>reasoning about fundamental tradeoffs in parameter settings is difficult【推理参数的调整是很难的，不成体系】</li>\n<li>there is no theoretical understanding of what happens when Allegro senders with different utility functions interact with each other【异构发送者的互动？】</li>\n</ul>\n<img src=\"/2022/11/07/%E3%80%8C%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95%E3%80%8DPCC%20Vivace%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20221107021420840.png\" alt=\"image-20221107021420840\" style=\"zoom:50%;\">\n<p>Second, Allegro inherently ignores the information reflected in the utility when deciding on step size</p>\n<ul>\n<li>ε是固定的，大了难以收敛，小了收敛太慢</li>\n<li>没有充分利用utility 反应的信息</li>\n</ul>\n<img src=\"/2022/11/07/%E3%80%8C%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95%E3%80%8DPCC%20Vivace%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20221107021351635.png\" alt=\"image-20221107021351635\" style=\"zoom: 50%;\">\n</blockquote>\n<p>为了解决15年PCC模型的问题，Vivace使用了在线学习+凸优化的思想，重新实现PCC高层架构的两个关键部分：（1）效用函数框架，以及（2）学习速率控制算法，达到了一个比较好的效果：</p>\n<blockquote>\n<ul>\n<li>\n<p>First, Vivace relies on a new, learning-theory-informed framework for utility derivation [12], which guarantees multiple competing Vivace senders will converge to a unique stable rate configuration that is fair and near-optimal.【保证多个异构决策者会收敛到一个公平点】</p>\n</li>\n<li>\n<p>Second, Vivace employs provably optimal gradient-ascent-based no-regret online optimization [37] to adjust sending rates, taking into account not only the direction (increase/decrease) that is more beneficial utility-wise, but also the extent to which increasing/decreasing the rate impacts utility.【基于梯度上升+无悔在线优化调整发送速率，保证收敛速度】</p>\n</li>\n</ul>\n</blockquote>\n<p>之前一直提到无悔学习，或者说无遗憾学习（no-regret learning）,之前学经管的时候做过一点最小后悔值的计算，但对于无悔学习的收敛性没有认识，这里我补充了一些背景知识（<strong>非常粗浅的了解，还有待学习</strong>）【（选择更好的u这里体现了无悔？】</p>\n<blockquote>\n<p>一般来讲，采用基于后悔值的学习方法以后，每个智能体根据各个行为的后悔值做出行为选择。如果一种算法能够保证最大后悔值渐进的变为零，那么该种算法就可以被称作无悔学习算法。<br>\n最著名的无悔学习算法就是后悔匹配算法(regret matching)，在每一个决策时刻，每个智能体按照每个行为的后悔值的概率做出选择，即具有最大概率的行为被选中的机会越大。在多智能体系统中，如果所有的智能体都采用相同的无悔学习算法，例如后悔匹配算法，那么所有智能体的联合行为将渐进地收敛于一组无悔点。换句话说，一组无悔点也可以被看作一种理想情况或者说一种高效的运行环境。在无悔点集合上，每个智能体所得到的平均回报不少于其它行为所能产生的回报。</p>\n</blockquote>\n<p>作者在文章中分析了无悔学习的优势与劣势（实际上按作者的描述这个劣势对于拥塞控制算法影响并不大）</p>\n<blockquote>\n<p>优势：</p>\n<ul>\n<li>no-regret provides a formal performance  guarantee for individual senders across all network conditions【在所有的网络环境下，无悔学习对每个发送者<strong>正式地</strong>保证了性能】</li>\n<li>no-regret provides a powerful lens for <strong>theoretical analysis</strong>, which we will use to reason formally about convergence with multiple competing no-regret senders, even with heterogeneous utility functions across the senders, and also about tradeoffs between resilienceto non-congestion loss and loss upon convergence.【无悔学习提供了关于异构的PCC发送者的收敛性理论分析的依据，并且提供了关于稳定性和损失率的平衡的分析方法】</li>\n</ul>\n<p>劣势：</p>\n<ul>\n<li>动态环境中，无悔算法所逼近的最佳策略是不断变化的，如果逼近的太慢那这个调整就没有意义了。如果遗憾在T个时间单位内消失到一个理想的低值，那么相对于每T个时间单位内的最佳固定策略而言，无遗憾保证就适用。</li>\n</ul>\n</blockquote>\n<h2 id=\"3-vivace’s-utility-framework\">3 Vivace’s Utility Framework<a title=\"#3-vivace’s-utility-framework\" href=\"#3-vivace’s-utility-framework\"></a></h2>\n<p>![image-20221106141724415](「拥塞控制算法」PCC Vivace阅读笔记/image-20221106141724415.png)</p>\n<p>为什么要用梯度呢？</p>\n<blockquote>\n<p>A single sender on a link with a large buffer sends at a rate of twice the capacity of the link for a single MI; then, in the next MI, it tries a slightly lower but still over-capacity rate. Such a sender would experience higher absolute latency in the second MI than in the first MI (since the link’s queue is only further lengthened), even though lowering the rate was clearly the right choice.</p>\n<p><strong>To learn within a single MI</strong> that lowering the rate is more beneficial, the sender examines the rate at which latency increases or decreases.【加入latency的考虑】</p>\n</blockquote>\n<h3 id=\"3.1-stability-and-fairness\">3.1 Stability and Fairness<a title=\"#3.1-stability-and-fairness\" href=\"#3.1-stability-and-fairness\"></a></h3>\n<p>在给出Vivace采用的效用方程后，Vivace在此基础上展开了稳定性和公平性的分析，作者先引入了博弈论中关于凹函数的理论（如果一个可微函数f它的导数f’在某区间是单调递减的，f就是凹的)</p>\n<blockquote>\n<p>When t ≤ 1, the family of utility functions in Equation 1 falls into the category of “<strong>socially-concave</strong>” in <strong>game theory</strong> [12].</p>\n</blockquote>\n<p>这里利用了博弈论的理论证明了：一组严格凹函数下的博弈会陷入一组纳什均衡点。再配合上凸优化的梯度上升理论+作者自己搞的信心模块，就保证了收敛的速度也很快。</p>\n<blockquote>\n<ul>\n<li>\n<p>guarantees high performance from the individual sender’s perspective</p>\n</li>\n<li>\n<p>ensures quick convergence to a global rateconfiguration</p>\n</li>\n</ul>\n</blockquote>\n<p>作者预设了一个场景：N个发送者竞争一个具有瓶颈的链路</p>\n<blockquote>\n<p>Theorem 1. When n Vivace-senders share a bottleneck link, and each Vivace-sender i’s utility function is defined as in Eq. 1, the senders’ sending rates converge to a fixed configuration (x∗1, . . . ,x∗ n) such that x∗ 1 = x∗ 2 = . . . = x∗n</p>\n<p>Theorem 2. Let C denote the capacity of the bottleneck link. If b ≥ tn^2−t^ C^t−1^, then the latency in the unique stable configuration is the base RTT.</p>\n</blockquote>\n<h3 id=\"3.2-random-loss-vs.-congestion\">3.2 Random Loss vs. Congestion<a title=\"#3.2-random-loss-vs.-congestion\" href=\"#3.2-random-loss-vs.-congestion\"></a></h3>\n<blockquote>\n<p>In a system of n Vivace senders, each ploss-resilient, the loss rate L of each sender i in equilibrium (with no random loss) satisfies:</p>\n<p>![image-20221107171538989](「拥塞控制算法」PCC Vivace阅读笔记/image-20221107171538989.png)</p>\n<img src=\"/2022/11/07/%E3%80%8C%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95%E3%80%8DPCC%20Vivace%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20221107171724313.png\" alt=\"image-20221107171724313\" style=\"zoom:50%;\">\n<p>推论：When n → ∞, the congestion loss rate on convergence approaches the random loss resilience p!【发送者趋近于n时损失率趋近设定的忍受率】</p>\n<p>推论：withstanding more random loss comes at the cost of suffering more loss upon convergence for a large number of senders.【随机损失的忍受是以收敛时的损失率为代价的】</p>\n</blockquote>\n<h3 id=\"3.3-heterogeneous-senders\">3.3 Heterogeneous Senders<a title=\"#3.3-heterogeneous-senders\" href=\"#3.3-heterogeneous-senders\"></a></h3>\n<p>Vivace可以通过修改异构发送者的效用方程达到控制链路占用量的效果，这对于未来的SDN是有好处的。</p>\n<blockquote>\n<p>Vivace’s utility function framework, in contrast, provides flexibility in resource-allocation. 【ci调整了对损失率的惩罚值】</p>\n<p>![image-20221107180645983](「拥塞控制算法」PCC Vivace阅读笔记/image-20221107180645983.png)</p>\n</blockquote>\n<p>不过我猜异构发送者不支持时延based，不然就不会只写一个loss-based的了。</p>\n<blockquote>\n<p>Hence, one can flexibly adjust the bandwidth allocated to each sender at equilibrium by tuning Vivace’s param-<br>\neters {ci}</p>\n<p><svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1.807ex\" height=\"2.009ex\" style=\"vertical-align: -0.671ex;\" viewbox=\"0 -576.1 777.8 865.1\" role=\"img\" focusable=\"false\" xmlns=\"http://www.w3.org/2000/svg\" aria-labelledby=\"MathJax-SVG-1-Title\">\n<title id=\"MathJax-SVG-1-Title\">c_{i}</title>\n<defs aria-hidden=\"true\">\n<path stroke-width=\"1\" id=\"E1-MJMATHI-63\" d=\"M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z\"/>\n<path stroke-width=\"1\" id=\"E1-MJMATHI-69\" d=\"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z\"/>\n</defs>\n<g stroke=\"currentColor\" fill=\"currentColor\" stroke-width=\"0\" transform=\"matrix(1 0 0 -1 0 0)\" aria-hidden=\"true\">\n <use xlink:href=\"#E1-MJMATHI-63\" x=\"0\" y=\"0\"/>\n <use transform=\"scale(0.707)\" xlink:href=\"#E1-MJMATHI-69\" x=\"613\" y=\"-213\"/>\n</g>\n</svg>参数的设置方法：</p>\n<p>![image-20221107180748101](「拥塞控制算法」PCC Vivace阅读笔记/image-20221107180748101.png)</p>\n</blockquote>\n<h2 id=\"4-vivace’s-rate-control\">4 Vivace’s Rate Control<a title=\"#4-vivace’s-rate-control\" href=\"#4-vivace’s-rate-control\"></a></h2>\n<p>最开始慢启动，然后在效用第一次减小后进入速率控制阶段（不再退出）</p>\n<h3 id=\"4.1-key-idea-and-challenges\">4.1 Key Idea and Challenges<a title=\"#4.1-key-idea-and-challenges\" href=\"#4.1-key-idea-and-challenges\"></a></h3>\n<blockquote>\n<p>when the utility functions are strictly convex, which is satisfied when t &lt; 1 in our utility function formulation</p>\n</blockquote>\n<p>作者指出：满足上面条件的效用方程可以保证两件事情：（本文老生常谈的话题了）</p>\n<blockquote>\n<ul>\n<li>Each sender is guaranteed that employing Vivace is (asymptotically) no worse than the optimal fixed sending rate<br>\nin hindsight【即使在极度变化的环境也适用】</li>\n<li>When multiple senders share the same link, quick convergence to an equilibrium point is guaranteed</li>\n</ul>\n</blockquote>\n<p>所以具体怎么控制呢，其实就是把原本PCC的步长修改了一下。以前的步长是ε的倍数，收敛效果不好。现在加了点凸优化对步长的调优方案——梯度（这背后的理论是无悔选择）</p>\n<h3 id=\"4.2-translating-utility-gradients-to-rates\">4.2 Translating Utility Gradients to Rates<a title=\"#4.2-translating-utility-gradients-to-rates\" href=\"#4.2-translating-utility-gradients-to-rates\"></a></h3>\n<p>先测试两个MI的u值，然后计算梯度，最后给梯度乘一个系数</p>\n<p>![image-20221107185845223](「拥塞控制算法」PCC Vivace阅读笔记/image-20221107185845223.png)</p>\n<blockquote>\n<p>![image-20221107182159933](「拥塞控制算法」PCC Vivace阅读笔记/image-20221107182159933.png)\t![image-20221107182321850](「拥塞控制算法」PCC Vivace阅读笔记/image-20221107182321850.png)![image-20221107182404003](「拥塞控制算法」PCC Vivace阅读笔记/image-20221107182404003.png)</p>\n</blockquote>\n<p>当然，光是这样收敛速度还是不太够，作者又加上了PCC的老思路——信心模块。对于同向的增长，我们让他加大力度。对于突然变向的情况，我们把信心模块重置。</p>\n<blockquote>\n<p>![image-20221107182504988](「拥塞控制算法」PCC Vivace阅读笔记/image-20221107182504988.png)</p>\n</blockquote>\n<p>接着，作者提出了一个异常处理的机制——梯度可能会因为偶发因素而变得异常高，所以需要给步长加一个限定值</p>\n<blockquote>\n<p>Whenever Vivace’s computed rate change (∆r) exceeds ωr, the effective rate change is capped at ωr</p>\n</blockquote>\n<p>这个限定值是动态变化的，Δr &gt; wr 那么w就变大，Δr &lt; wr 那么w就变小，如果速率控制的方向变化了w就重置：</p>\n<blockquote>\n<ul>\n<li>Specifically, ω is updated to ω = ω0 +k · δ following k consecutive rate adjustments in which the gradient-based rate-change ∆r exceeded the dynamic change boundary,</li>\n<li>Whenever ∆r ≤r·ω, Vivace recalibrates the value of k in the formula ω=ω0+k·δ to be the smallest value<br>\nfor which ∆r ≤ rω.</li>\n<li>k is reset to 0 when the direction of rate adjustment changes</li>\n</ul>\n</blockquote>\n<h3 id=\"4.3-contending-with-unreliable-statistics【tricks，其实也是引入了硬连接】\">4.3 Contending with Unreliable Statistics【tricks，其实也是引入了硬连接】<a title=\"#4.3-contending-with-unreliable-statistics【tricks，其实也是引入了硬连接】\" href=\"#4.3-contending-with-unreliable-statistics【tricks，其实也是引入了硬连接】\"></a></h3>\n<h4 id=\"estimating-the-rtt-gradient-via-linear-regression.\">Estimating the RTT gradient via linear regression.<a title=\"#estimating-the-rtt-gradient-via-linear-regression.\" href=\"#estimating-the-rtt-gradient-via-linear-regression.\"></a></h4>\n<p>一言以蔽之：用线性回归求RTT的梯度【上周讲错了x_x】</p>\n<h4 id=\"low-pass-filtering-of-rtt-gradient.\">Low-pass filtering of RTT gradient.<a title=\"#low-pass-filtering-of-rtt-gradient.\" href=\"#low-pass-filtering-of-rtt-gradient.\"></a></h4>\n<p>使用低通滤波机制忽略小的、短暂的延时抖动【(&gt; 0.01)的抖动才纳入计算】</p>\n<h4 id=\"double-checking-abnormal-measurements.\">Double checking abnormal measurements.<a title=\"#double-checking-abnormal-measurements.\" href=\"#double-checking-abnormal-measurements.\"></a></h4>\n<p>这个补丁是对一种奇怪的现象（偶尔，测量结果会导致 &quot;反直觉 &quot;的观察）进行处理——发送速率升高反而延时降低，作者认为这是一个不太可能发生的事情，所以要recheck一次。如果结果显示确实是这样的，那就把doublecheck的Δr 取个平均。</p>\n<img src=\"/2022/11/07/%E3%80%8C%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95%E3%80%8DPCC%20Vivace%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20221107190005677.png\" alt=\"image-20221107190005677\" style=\"zoom:50%;\">\n<blockquote>\n<p>even with complex conditions it is highly unlikely that sending faster is the cause of lower loss; more likely, this is due to measurement noise or changing conditions (e.g., another sender reducing its rate).</p>\n</blockquote>\n<h4 id=\"mi-timeout.\">MI timeout.<a title=\"#mi-timeout.\" href=\"#mi-timeout.\"></a></h4>\n<p>一个MI窗口的信息大概应该在一个RTT返回回来，如果很久都没回来，说明网络出问题了，所以当超过Timeout时间还没有拿到足够的一个MI的RTT信息，Vivace就会把发送速率减半。</p>\n<h3 id=\"4.4-tcp-friendliness\">4.4 TCP Friendliness<a title=\"#4.4-tcp-friendliness\" href=\"#4.4-tcp-friendliness\"></a></h3>\n<p>作者认为loss-based 的算法是必然会抢占TCP的带宽的，而Latency based的算法又因为TCP系列的算法是lose-based的，对延时不感兴趣，所有当延时上升的时候，latency based的算法主动谦让，TCP系列得寸进尺，延时维持高位，latency based的算法就被TCP系列支配了。</p>\n<p>所有类似PCC Vivace这样混合型的算法可能可以解决这个困境。作者预设了两个场景：</p>\n<blockquote>\n<ul>\n<li>a Vivace sender is the only sender on a certain link. It tries out two rates that exceed the link’s bandwidth, and the buffer for that link is not yet full.【not full的链，纯Vivace单流】</li>\n<li>the Vivace sender is sharing a link that is already heavily utilized by many loss-based protocols like TCP CUBIC and the buffer is, consequently, almost always full.【full 链， 多TCP+单Vivace】</li>\n</ul>\n</blockquote>\n<p>经过分析，前者把latency纳入了考虑，后者由于latency在此时变化不大，所以退化为了loss-based去和TCP抢带宽了（PCC Allegro）</p>\n<p>但我还是有几点疑虑：</p>\n<ul>\n<li>full链，多Vivace流会怎么样？是否也会退化？退化以后的收敛性如何？</li>\n<li>full链，多TCP单Vivace中，Vivace作为loss-based抢占能力如何？退化以后的收敛性如何？</li>\n</ul>\n<h2 id=\"5-implementation-and-evaluation\">5 Implementation and Evaluation<a title=\"#5-implementation-and-evaluation\" href=\"#5-implementation-and-evaluation\"></a></h2>\n<h3 id=\"latency-awareness-(100mbps,-30ms-rtt-emulab-bottleneck-link)\">Latency awareness (100Mbps, 30ms RTT Emulab bottleneck link)<a title=\"#latency-awareness-(100mbps,-30ms-rtt-emulab-bottleneck-link)\" href=\"#latency-awareness-(100mbps,-30ms-rtt-emulab-bottleneck-link)\"></a></h3>\n<p>PCC Vivace很好地解决了缓冲区膨胀的问题，因为他是一个考虑了时延的算法</p>\n<img src=\"/2022/11/07/%E3%80%8C%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95%E3%80%8DPCC%20Vivace%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20221107184034911.png\" alt=\"image-20221107184034911\" style=\"zoom:50%;\">\n<h3 id=\"rapid-reaction-to-network-changes-(10-100mbps,-10-100ms-rtt，0-1%-random-loss)\">Rapid reaction to network changes (10-100Mbps, 10-100ms RTT，0-1% random loss)<a title=\"#rapid-reaction-to-network-changes-(10-100mbps,-10-100ms-rtt，0-1%-random-loss)\" href=\"#rapid-reaction-to-network-changes-(10-100mbps,-10-100ms-rtt，0-1%-random-loss)\"></a></h3>\n<ul>\n<li>\n<p>CUBIC不能应对random loss，所以维持低位</p>\n</li>\n<li>\n<p>BBR不能快速应对RTT的变化【因为BBR中probeRTT、probeBDP是不同时间进行的？】</p>\n</li>\n<li>\n<p>Allegro不能在瓶颈带宽下降的时候快速减小发送速率——这会导致丢包</p>\n</li>\n</ul>\n<img src=\"/2022/11/07/%E3%80%8C%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95%E3%80%8DPCC%20Vivace%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20221107184403449.png\" alt=\"image-20221107184403449\" style=\"zoom:50%;\">\n<h3 id=\"fair-equilibrium-(100mbps,-30ms-rtt,-75kb-buffer)\">Fair equilibrium (100Mbps, 30ms RTT, 75KB buffer)<a title=\"#fair-equilibrium-(100mbps,-30ms-rtt,-75kb-buffer)\" href=\"#fair-equilibrium-(100mbps,-30ms-rtt,-75kb-buffer)\"></a></h3>\n<p>肉眼可见，PCC Vivace的收敛性（稳定、速度）更好</p>\n<img src=\"/2022/11/07/%E3%80%8C%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95%E3%80%8DPCC%20Vivace%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20221107184906714.png\" alt=\"image-20221107184906714\" style=\"zoom:50%;\">\n<h3 id=\"tcp-friendliness\">TCP Friendliness<a title=\"#tcp-friendliness\" href=\"#tcp-friendliness\"></a></h3>\n<ul>\n<li>BBR占据接近15%的带宽（不管有多少个TCP竞争者）</li>\n<li>PCC Vivace在网络不拥堵（发送者少）的情况下，抢不过TCP，当TCP发送者多起来的时候，PCC Vivace可以抢资源</li>\n</ul>\n<img src=\"/2022/11/07/%E3%80%8C%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95%E3%80%8DPCC%20Vivace%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20221107185109029.png\" alt=\"image-20221107185109029\" style=\"zoom:50%;\">\n<h3 id=\"flexible-equilibrium-by-tuning-utility-knobs\">Flexible equilibrium by tuning utility knobs<a title=\"#flexible-equilibrium-by-tuning-utility-knobs\" href=\"#flexible-equilibrium-by-tuning-utility-knobs\"></a></h3>\n<p>通过调整Ci，可以修改PCC Vivace流在带宽中的占比（纯PCC Vivace竞争）</p>\n<p>![image-20221107185512324](「拥塞控制算法」PCC Vivace阅读笔记/image-20221107185512324.png)</p>\n<img src=\"/2022/11/07/%E3%80%8C%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95%E3%80%8DPCC%20Vivace%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20221107185430781.png\" alt=\"image-20221107185430781\" style=\"zoom: 67%;\">\n<h3 id=\"limitation-in-extremely-dynamic-networks\">Limitation in Extremely Dynamic Networks<a title=\"#limitation-in-extremely-dynamic-networks\" href=\"#limitation-in-extremely-dynamic-networks\"></a></h3>\n<p>LTE 有待改进的地方：可能是因为在<strong>急剧</strong>变化的网络下Vivace计算的结果来的不够及时。</p>\n<img src=\"/2022/11/07/%E3%80%8C%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95%E3%80%8DPCC%20Vivace%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/image-20221107191051065.png\" alt=\"image-20221107191051065\" style=\"zoom:67%;\">","next":{"title":"「编译原理」语法分析--进阶内容","link":"2022/11/03/「编译原理」语法分析-进阶内容"},"plink":"http://blog.horik.cn/2022/11/07/「拥塞控制算法」PCC Vivace阅读笔记/","toc":[{"id":"abstract","title":"Abstract","index":"1"},{"id":"1.introduction","title":"1.Introduction","index":"2"},{"id":"2.rate-control-through-online-learning","title":"2.Rate-Control Through Online Learning","index":"3"},{"id":"3-vivace’s-utility-framework","title":"3 Vivace’s Utility Framework","index":"4","children":[{"id":"3.1-stability-and-fairness","title":"3.1 Stability and Fairness","index":"4.1"},{"id":"3.2-random-loss-vs.-congestion","title":"3.2 Random Loss vs. Congestion","index":"4.2"},{"id":"3.3-heterogeneous-senders","title":"3.3 Heterogeneous Senders","index":"4.3"}]},{"id":"4-vivace’s-rate-control","title":"4 Vivace’s Rate Control","index":"5","children":[{"id":"4.1-key-idea-and-challenges","title":"4.1 Key Idea and Challenges","index":"5.1"},{"id":"4.2-translating-utility-gradients-to-rates","title":"4.2 Translating Utility Gradients to Rates","index":"5.2"},{"id":"4.3-contending-with-unreliable-statistics【tricks，其实也是引入了硬连接】","title":"4.3 Contending with Unreliable Statistics【tricks，其实也是引入了硬连接】","index":"5.3","children":[{"id":"estimating-the-rtt-gradient-via-linear-regression.","title":"Estimating the RTT gradient via linear regression.","index":"5.3.1"},{"id":"low-pass-filtering-of-rtt-gradient.","title":"Low-pass filtering of RTT gradient.","index":"5.3.2"},{"id":"double-checking-abnormal-measurements.","title":"Double checking abnormal measurements.","index":"5.3.3"},{"id":"mi-timeout.","title":"MI timeout.","index":"5.3.4"}]},{"id":"4.4-tcp-friendliness","title":"4.4 TCP Friendliness","index":"5.4"}]},{"id":"5-implementation-and-evaluation","title":"5 Implementation and Evaluation","index":"6","children":[{"id":"latency-awareness-(100mbps,-30ms-rtt-emulab-bottleneck-link)","title":"Latency awareness (100Mbps, 30ms RTT Emulab bottleneck link)","index":"6.1"},{"id":"rapid-reaction-to-network-changes-(10-100mbps,-10-100ms-rtt，0-1%-random-loss)","title":"Rapid reaction to network changes (10-100Mbps, 10-100ms RTT，0-1% random loss)","index":"6.2"},{"id":"fair-equilibrium-(100mbps,-30ms-rtt,-75kb-buffer)","title":"Fair equilibrium (100Mbps, 30ms RTT, 75KB buffer)","index":"6.3"},{"id":"tcp-friendliness","title":"TCP Friendliness","index":"6.4"},{"id":"flexible-equilibrium-by-tuning-utility-knobs","title":"Flexible equilibrium by tuning utility knobs","index":"6.5"},{"id":"limitation-in-extremely-dynamic-networks","title":"Limitation in Extremely Dynamic Networks","index":"6.6"}]}],"reward":true,"copyright":{"author":"Horik","link":"<a href=\"http://blog.horik.cn/2022/11/07/「拥塞控制算法」PCC Vivace阅读笔记/\" title=\"「拥塞控制算法」PCC Vivace阅读笔记\">http://blog.horik.cn/2022/11/07/「拥塞控制算法」PCC Vivace阅读笔记/</a>","license":"Attribution-NonCommercial-NoDerivatives 4.0 International (<a href=\\\"https://creativecommons.org/licenses/by-nc-sa/4.0/\\\" rel=\\\"external nofollow\\\" target=\\\"_blank\\\">CC BY-NC-ND 4.0</a>)","published":"November 7, 2022","updated":"November 8, 2022"},"reading_time":"4790 words in 32 min"}